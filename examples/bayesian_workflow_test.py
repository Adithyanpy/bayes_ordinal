#!/usr/bin/env python3
"""
Bayesian Workflow Test for Ordinal Regression

This script implements a comprehensive Bayesian workflow following the structure
outlined in the Bayesian workflow paper (2011.01808v1.pdf). It compares logit
(cumulative) and partial odds models for ordinal regression.

Workflow Structure:
1. Data preparation and exploration
2. Model specification and prior elicitation
3. Prior predictive checks
4. Model fitting and diagnostics
5. Posterior predictive checks
6. Model comparison and selection
7. Computational issue resolution (if needed)
8. Model modification and refinement
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pymc as pm
import arviz as az
import bayes_ordinal as bo
from typing import Dict, Any, List

# Set random seed for reproducibility
np.random.seed(42)

# Configure plotting
plt.style.use('default')
sns.set_palette("husl")

def main():
    """
    Main Bayesian workflow function.
    """
    print("ðŸš€ BAYESIAN WORKFLOW FOR ORDINAL REGRESSION")
    print("=" * 60)
    print("Following 2011.01808v1.pdf structure")
    print()
    
    # ============================================================================
    # 1. DATA PREPARATION AND EXPLORATION
    # ============================================================================
    print("1ï¸âƒ£ DATA PREPARATION AND EXPLORATION")
    print("-" * 40)
    
    # Load and prepare data
    data = load_and_prepare_data()
    y_ordinal, X_features, K, feature_names = data
    
    print(f"âœ“ Data prepared: {len(y_ordinal)} observations, {K} categories")
    print(f"âœ“ Features: {X_features.shape[1]} variables")
    print(f"âœ“ Category distribution: {np.bincount(y_ordinal)}")
    
    # ============================================================================
    # 2. MODEL SPECIFICATION AND PRIOR ELICITATION
    # ============================================================================
    print("\n2ï¸âƒ£ MODEL SPECIFICATION AND PRIOR ELICITATION")
    print("-" * 50)
    
    # Specify priors
    priors_conservative = bo.get_conservative_priors(K=K, model_type="ordinal")
    priors_data_informed = bo.get_data_informed_priors(y_ordinal, X_features, K)
    
    print("âœ“ Conservative priors specified:")
    for key, value in priors_conservative.items():
        print(f"  {key}: {value}")
    
    print("âœ“ Data-informed priors specified:")
    for key, value in priors_data_informed.items():
        print(f"  {key}: {value}")
    
    # Create models
    models = create_models(y_ordinal, X_features, K, priors_conservative)
    print(f"âœ“ Models created: {list(models.keys())}")
    
    # ============================================================================
    # 2.5. CAUSAL GRAPH (DAG) SPECIFICATION
    # ============================================================================
    print("\n2.5ï¸âƒ£ CAUSAL GRAPH (DAG) SPECIFICATION")
    print("-" * 35)
    
    # Draw causal graph for the ordinal regression problem
    try:
        bo.draw_causal_graph(
            model_type="ordinal",
            feature_names=feature_names
        )
        print("âœ“ Causal graph (DAG) created")
    except Exception as e:
        print(f"âœ— Causal graph creation failed: {e}")
    
    # ============================================================================
    # 3. PRIOR PREDICTIVE CHECKS
    # ============================================================================
    print("\n3ï¸âƒ£ PRIOR PREDICTIVE CHECKS")
    print("-" * 30)
    
    prior_results = {}
    for model_name, model in models.items():
        print(f"\nRunning prior predictive for {model_name}...")
        try:
            prior_idata = bo.run_prior_predictive(model, draws=100, include_r2=True, y_obs=y_ordinal)
            prior_results[model_name] = prior_idata
            print(f"âœ“ Prior predictive completed for {model_name}")
            
            # Prior predictive plots are handled by run_prior_predictive function
            # No need for additional plotting here
            
        except Exception as e:
            print(f"âœ— Prior predictive failed for {model_name}: {e}")
    
    # ============================================================================
    # 4. MODEL FITTING AND DIAGNOSTICS
    # ============================================================================
    print("\n4ï¸âƒ£ MODEL FITTING AND DIAGNOSTICS")
    print("-" * 35)
    
    fitted_models = {}
    for model_name, model in models.items():
        print(f"\nFitting {model_name}...")
        try:
            # Fit model
            idata = bo.fit_model(
                model, 
                draws=1000, 
                tune=500, 
                chains=4,
                sampler="nuts"
            )
            fitted_models[model_name] = idata
            print(f"âœ“ {model_name} fitted successfully")
            
            # Run diagnostics
            diagnostics = bo.summarize_diagnostics(idata)
            print(f"âœ“ Diagnostics completed for {model_name}")
            
            # Check convergence
            convergence = bo.check_convergence(idata)
            print(f"âœ“ Convergence check: {convergence['converged']}")
            
            # Check computational issues
            issues = bo.diagnose_computational_issues(idata)
            if issues['recommendations']:
                print(f"âš ï¸  Issues detected: {issues['recommendations']}")
            
        except Exception as e:
            print(f"âœ— Model fitting failed for {model_name}: {e}")
    
    if not fitted_models:
        print("âœ— No models fitted successfully")
        return
    
    # ============================================================================
    # 4.5. SENSITIVITY ANALYSIS
    # ============================================================================
    print("\n4.5ï¸âƒ£ SENSITIVITY ANALYSIS")
    print("-" * 25)
    
    for model_name, idata in fitted_models.items():
        print(f"\nRunning sensitivity analysis for {model_name}...")
        
        try:
            # Prior sensitivity analysis
            print(f"  Testing prior sensitivity for {model_name}...")
            bo.prior_sensitivity(
                build_model=lambda y, X, K, priors: models[model_name],
                y=y_ordinal,
                X=X_features,
                K=K,
                hyper_name="coef_sigma",
                hyper_values=[0.5, 1.0, 2.0, 5.0],
                trace_var="beta",
                draws=500,
                tune=200,
                chains=2
            )
            print(f"  âœ“ Prior sensitivity completed for {model_name}")
            
            # Influence diagnostics
            print(f"  Testing influence diagnostics for {model_name}...")
            bo.plot_influential(idata, threshold=0.7)
            print(f"  âœ“ Influence diagnostics completed for {model_name}")
            
        except Exception as e:
            print(f"  âœ— Sensitivity analysis failed for {model_name}: {e}")
    
    # ============================================================================
    # 5. POSTERIOR PREDICTIVE CHECKS
    # ============================================================================
    print("\n5ï¸âƒ£ POSTERIOR PREDICTIVE CHECKS")
    print("-" * 35)
    
    for model_name, idata in fitted_models.items():
        print(f"\nRunning posterior predictive for {model_name}...")
        try:
            ppc_idata = bo.run_posterior_predictive(models[model_name], idata)
            print(f"âœ“ Posterior predictive completed for {model_name}")
            
            # Posterior predictive plots are handled by run_posterior_predictive function
            # No need for additional plotting here
            
        except Exception as e:
            print(f"âœ— Posterior predictive failed for {model_name}: {e}")
    
    # ============================================================================
    # 6. MODEL COMPARISON AND SELECTION
    # ============================================================================
    print("\n6ï¸âƒ£ MODEL COMPARISON AND SELECTION")
    print("-" * 35)
    
    try:
        # Basic model comparison
        comparison = bo.compare_models(models, fitted_models, ic="loo")
        print("âœ“ Basic model comparison completed")
        print(f"Best model: {comparison.index[0]}")
        
        # Advanced model comparison with stacking
        stacking_comparison = bo.compare_models_stacking(models, fitted_models)
        print("âœ“ Stacking comparison completed")
        
        # McElreath-style interpretation
        interpretation = bo.compare_models_interpretation(models, fitted_models)
        print("âœ“ Model interpretation completed")
        
        # Plot model comparison
        bo.plot_model_comparison_interpretation(fitted_models)
        plt.show()
        
    except Exception as e:
        print(f"âœ— Model comparison failed: {e}")
    
    # ============================================================================
    # 7. COMPUTATIONAL ISSUE RESOLUTION
    # ============================================================================
    print("\n7ï¸âƒ£ COMPUTATIONAL ISSUE RESOLUTION")
    print("-" * 35)
    
    for model_name, idata in fitted_models.items():
        print(f"\nChecking computational issues for {model_name}...")
        
        # Comprehensive computation check
        comp_check = bo.comprehensive_computation_check(
            models[model_name], idata, y_ordinal, X_features
        )
        
        if comp_check.get('issues_found', False):
            print(f"âš ï¸  Issues found in {model_name}:")
            for issue in comp_check.get('recommendations', []):
                print(f"  - {issue}")
            
            # Try to resolve issues
            try:
                # Check for multimodality
                multimodality = bo.check_multimodality(idata)
                if multimodality.get('has_multimodality', False):
                    print(f"  - Multimodality detected in {model_name}")
                
                # Try reparameterization
                reparam_model = bo.reparameterize_model(models[model_name], "non_centered")
                print(f"  - Reparameterized {model_name}")
                
            except Exception as e:
                print(f"  - Could not resolve issues: {e}")
        else:
            print(f"âœ“ No computational issues detected in {model_name}")
    
    # ============================================================================
    # 8. MODEL MODIFICATION AND REFINEMENT
    # ============================================================================
    print("\n8ï¸âƒ£ MODEL MODIFICATION AND REFINEMENT")
    print("-" * 35)
    
    try:
        # Pick best model for refinement
        best_model_name = list(fitted_models.keys())[0]  # Use first as best for now
        best_model = models[best_model_name]
        
        print(f"Refining {best_model_name}...")
        
        # Iterative model refinement
        refinement_result = bo.iterative_model_refinement(
            best_model, y_ordinal, X_features, K, max_iterations=2
        )
        print("âœ“ Iterative refinement completed")
        
        # Multiverse analysis
        model_specs = [
            {'model_type': 'cumulative', 'link': 'logit'},
            {'model_type': 'partial_odds'}
        ]
        
        multiverse_result = bo.multiverse_analysis(
            model_specs, y_ordinal, X_features, K
        )
        print("âœ“ Multiverse analysis completed")
        
        # Model averaging
        averaging_result = bo.model_averaging_stacking(fitted_models)
        print("âœ“ Model averaging completed")
        
    except Exception as e:
        print(f"âœ— Model modification failed: {e}")
    
    # ============================================================================
    # 9. FINAL RESULTS AND VISUALIZATION
    # ============================================================================
    print("\n9ï¸âƒ£ FINAL RESULTS AND VISUALIZATION")
    print("-" * 35)
    
    # Create final visualizations
    for model_name, idata in fitted_models.items():
        print(f"\nCreating visualizations for {model_name}...")
        
        try:
            # Coefficient effects
            bo.plot_forest(idata, var_name="beta", feature_names=feature_names)
            plt.title(f'Coefficient Effects: {model_name}')
            plt.show()
            
            # Cutpoints (for cumulative model)
            if 'alpha' in idata.posterior:
                bo.plot_cutpoints(idata)
                plt.title(f'Cutpoints: {model_name}')
                plt.show()
            
            # Category probabilities
            bo.plot_category_probabilities(idata, models[model_name], X_features[:10])
            plt.title(f'Category Probabilities: {model_name}')
            plt.show()
            
            # Model structure
            bo.plot_model_structure(models[model_name])
            
            # Causal graph for this specific model
            try:
                bo.draw_causal_graph(
                    model=models[model_name],
                    feature_names=feature_names
                )
                print(f"  âœ“ Causal graph created for {model_name}")
            except Exception as e:
                print(f"  âœ— Causal graph failed for {model_name}: {e}")
            
        except Exception as e:
            print(f"âœ— Visualization failed for {model_name}: {e}")
    
    # ============================================================================
    # 10. WORKFLOW SUMMARY
    # ============================================================================
    print("\nðŸŽ‰ BAYESIAN WORKFLOW COMPLETED!")
    print("=" * 40)
    
    print(f"\nðŸ“Š SUMMARY:")
    print(f"  â€¢ Models compared: {len(models)}")
    print(f"  â€¢ Models fitted: {len(fitted_models)}")
    print(f"  â€¢ Prior predictive checks: {len(prior_results)}")
    print(f"  â€¢ Posterior predictive checks: {len(fitted_models)}")
    print(f"  â€¢ Model comparison: Completed")
    print(f"  â€¢ Computational checks: Completed")
    print(f"  â€¢ Model refinement: Completed")
    
    print(f"\nâœ… WORKFLOW COMPONENTS TESTED:")
    print(f"  â€¢ Data preparation and exploration")
    print(f"  â€¢ Model specification and prior elicitation")
    print(f"  â€¢ Causal graph (DAG) specification")
    print(f"  â€¢ Prior predictive checks")
    print(f"  â€¢ Model fitting and diagnostics")
    print(f"  â€¢ Sensitivity analysis (prior + influence)")
    print(f"  â€¢ Posterior predictive checks")
    print(f"  â€¢ Model comparison and selection")
    print(f"  â€¢ Computational issue resolution")
    print(f"  â€¢ Model modification and refinement")
    
    return fitted_models


def load_and_prepare_data():
    """
    Load and prepare the Wines2012 dataset.
    """
    try:
        # Load Wines2012 dataset
        wine_data = pd.read_csv("../Dataset/Wines2012.csv", sep=';')
        scores = wine_data['score'].values
        wine_amer = wine_data['wine.amer'].values
        judge_amer = wine_data['judge.amer'].values
        
        print(f"âœ“ Loaded Wines2012 dataset: {len(scores)} observations")
        
    except Exception as e:
        print(f"âœ— Data loading failed: {e}")
        # Create synthetic data for testing
        np.random.seed(42)
        n_samples = 200
        scores = np.random.normal(15, 2, n_samples)
        wine_amer = np.random.binomial(1, 0.5, n_samples)
        judge_amer = np.random.binomial(1, 0.5, n_samples)
        print(f"âœ“ Created synthetic data: {n_samples} observations")
    
    # Create ordinal categories from continuous scores
    score_quantiles = np.percentile(scores, [25, 50, 75])
    y_ordinal = np.zeros_like(scores, dtype=int)
    y_ordinal[scores > score_quantiles[2]] = 3  # High quality
    y_ordinal[(scores > score_quantiles[1]) & (scores <= score_quantiles[2])] = 2  # Medium-high
    y_ordinal[(scores > score_quantiles[0]) & (scores <= score_quantiles[1])] = 1  # Medium-low
    y_ordinal[scores <= score_quantiles[0]] = 0  # Low quality
    
    K = len(np.unique(y_ordinal))
    
    # Create feature matrix
    X_features = np.column_stack([
        wine_amer,          # Wine origin (American vs French)
        judge_amer          # Judge origin (American vs French)
    ])
    
    # Standardize features
    X_features, scaler = bo.standardize_features(X_features)
    
    feature_names = ["Wine Origin", "Judge Origin"]
    
    # Ensure feature names are strings
    feature_names = [str(name) for name in feature_names]
    
    return y_ordinal, X_features, K, feature_names


def create_models(y_ordinal, X_features, K, priors):
    """
    Create logit (cumulative) and partial odds models.
    """
    models = {}
    
    # Model 1: Logit (Cumulative) model
    try:
        models['logit'] = bo.cumulative_model(
            y_ordinal, X_features, K, 
            priors=priors,
            link="logit"
        )
        print("âœ“ Logit (cumulative) model created")
    except Exception as e:
        print(f"âœ— Logit model creation failed: {e}")
    
    # Model 2: Partial odds model
    try:
        models['partial_odds'] = bo.partial_odds_model(
            y_ordinal, X_features, K,
            priors=priors
        )
        print("âœ“ Partial odds model created")
    except Exception as e:
        print(f"âœ— Partial odds model creation failed: {e}")
    
    return models





if __name__ == "__main__":
    main() 